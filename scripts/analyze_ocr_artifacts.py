"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä OCR-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤

–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç raw_ocr —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å ground_truth –∏ –≤—ã—è–≤–ª—è–µ—Ç —Ä–∞–∑–ª–∏—á–∏—è.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ä—É—á–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/analyze_ocr_artifacts.py <receipt_id>

–ü—Ä–∏–º–µ—Ä:
    python scripts/analyze_ocr_artifacts.py IMG_1292
"""

import json
import sys
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from difflib import SequenceMatcher

# –ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
PROJECT_ROOT = Path("/Users/sergejevsukov/Downloads/Finpi_OCR")


class ArtifactAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç OCR –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—è —Å Ground Truth."""

    def __init__(self, receipt_id: str):
        self.receipt_id = receipt_id
        self.artifacts = []
        self.metadata = {}

    def load_data(self) -> Tuple[Optional[dict], Optional[dict]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç raw_ocr –∏ ground_truth –¥–∞–Ω–Ω—ã–µ."""
        # –ü–æ–∏—Å–∫ raw_ocr.json
        raw_ocr_paths = list(PROJECT_ROOT.glob(f"data/output/{self.receipt_id}*/raw_ocr.json"))
        if not raw_ocr_paths:
            print(f"‚ùå raw_ocr.json –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è {self.receipt_id}")
            return None, None

        raw_ocr_path = raw_ocr_paths[0]
        with open(raw_ocr_path, 'r', encoding='utf-8') as f:
            raw_ocr = json.load(f)

        # –ü–æ–∏—Å–∫ ground_truth
        gt_paths = list(PROJECT_ROOT.glob(f"docs/ground_truth/*{self.receipt_id}*.json"))
        if not gt_paths:
            print(f"‚ö†Ô∏è  ground_truth –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è {self.receipt_id}")
            return raw_ocr, None

        gt_path = gt_paths[0]
        with open(gt_path, 'r', encoding='utf-8') as f:
            gt_data = json.load(f)

        return raw_ocr, gt_data

    def extract_metadata(self, raw_ocr: dict, gt_data: dict):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ —á–µ–∫–∞."""
        locale = gt_data.get("locale", "unknown")
        store = gt_data.get("store", {}).get("brand", "unknown")
        source_file = raw_ocr.get("metadata", {}).get("source_file", self.receipt_id)

        self.metadata = {
            "locale": locale,
            "store": store,
            "source_file": source_file,
            "total_words": len(raw_ocr.get("words", [])),
            "total_items": len(gt_data.get("items", []))
        }

    def fuzzy_find_in_text(self, target: str, text: str, threshold: float = 0.6) -> Tuple[bool, str, float]:
        """
        –ò—â–µ—Ç target –≤ text —Å –Ω–µ—á—ë—Ç–∫–∏–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º.

        Returns:
            (found, matched_text, similarity)
        """
        target_clean = self.clean_text(target)
        text_clean = self.clean_text(text)

        if target_clean in text_clean:
            return True, target_clean, 1.0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥—Å—Ç—Ä–æ–∫–∏
        words_in_text = text_clean.split()
        for word in words_in_text:
            similarity = SequenceMatcher(None, target_clean.lower(), word.lower()).ratio()
            if similarity >= threshold:
                return True, word, similarity

        return False, "", 0.0

    def clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
        return re.sub(r'[\s\-\*\'"‚Äû"]+', '', text).lower()

    def analyze_item_names(self, raw_ocr: dict, gt_data: dict):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤."""
        ocr_text = raw_ocr.get("full_text", "")
        gt_items = gt_data.get("items", [])

        for item in gt_items:
            name = item.get("raw_name", item.get("name", ""))

            # –ò—â–µ–º –≤ OCR
            found, matched, similarity = self.fuzzy_find_in_text(name, ocr_text)

            if not found:
                # –°–ª–æ–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ - WORD_SPLIT –∏–ª–∏ CHAR_MISS
                self.artifacts.append({
                    "type": "WORD_SPLIT",
                    "ocr_text": "NOT FOUND",
                    "expected_text": name,
                    "context": name,
                    "suggestion": "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑—Ä—ã–≤ —Å–ª–æ–≤–∞ –∏–ª–∏ –ø–æ—Ç–µ—Ä—é —Å–∏–º–≤–æ–ª–æ–≤"
                })
            elif similarity < 1.0:
                # –ù–∞–π–¥–µ–Ω–æ —Å –æ—Ç–ª–∏—á–∏—è–º–∏
                differences = self.find_differences(name, matched)
                for diff in differences:
                    self.artifacts.append({
                        "type": diff["type"],
                        "ocr_text": matched,
                        "expected_text": name,
                        "context": matched,
                        "suggestion": diff["suggestion"]
                    })

    def find_differences(self, expected: str, actual: str) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–∏–ø—ã —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É —Å—Ç—Ä–æ–∫–∞–º–∏."""
        differences = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Ö–æ–∂–∏–µ —Å–∏–º–≤–æ–ª—ã
        char_pairs = [
            ("0", "O"), ("1", "l"), ("l", "1"), ("5", "S"), ("S", "5")
        ]
        for ocr_char, exp_char in char_pairs:
            if ocr_char in actual and exp_char in expected:
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏
                for i, (a, e) in enumerate(zip(actual, expected)):
                    if a == ocr_char and e == exp_char:
                        differences.append({
                            "type": "CHAR_SUB",
                            "suggestion": f"{ocr_char} ‚Üí {exp_char} –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {i}"
                        })
                        break

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
        if len(actual) != len(expected):
            if len(actual) < len(expected):
                differences.append({
                    "type": "CHAR_MISS",
                    "suggestion": f"–ü—Ä–æ–ø—É—Å–∫ —Å–∏–º–≤–æ–ª–æ–≤: –æ–∂–∏–¥–∞–µ–º {len(expected)} chars, –Ω–∞–π–¥–µ–Ω–æ {len(actual)}"
                })
            else:
                differences.append({
                    "type": "CHAR_EXTRA",
                    "suggestion": f"–õ–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã: –æ–∂–∏–¥–∞–µ–º {len(expected)} chars, –Ω–∞–π–¥–µ–Ω–æ {len(actual)}"
                })

        # –ï—Å–ª–∏ —Ä–∞–∑–ª–∏—á–∏—è –Ω–µ –Ω–∞—à–ª–∏ —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏
        if not differences:
            differences.append({
                "type": "UNKNOWN",
                "suggestion": "–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–æ–π –∞–Ω–∞–ª–∏–∑"
            })

        return differences

    def analyze_prices(self, raw_ocr: dict, gt_data: dict):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ü–µ–Ω—ã."""
        ocr_text = raw_ocr.get("full_text", "")
        gt_items = gt_data.get("items", [])

        for item in gt_items:
            price = item.get("total_price")
            if price is None:
                continue

            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω —Ü–µ–Ω—ã
            price_str = f"{price:.2f}"
            price_alt = price_str.replace(".", ",")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤ —Ç–µ–∫—Å—Ç–µ
            if price_str not in ocr_text and price_alt not in ocr_text:
                # –¶–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ - –≤–æ–∑–º–æ–∂–Ω–æ NUM_FMT
                self.artifacts.append({
                    "type": "NUM_FMT",
                    "ocr_text": "NOT FOUND",
                    "expected_text": price_str,
                    "context": f"–¢–æ–≤–∞—Ä: {item.get('raw_name', '?')}",
                    "suggestion": "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è (–∑–∞–ø—è—Ç–∞—è/—Ç–æ—á–∫–∞)"
                })

    def analyze_garbage(self, raw_ocr: dict):
        """–ò—â–µ—Ç –º—É—Å–æ—Ä–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã."""
        text = raw_ocr.get("full_text", "")

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –º—É—Å–æ—Ä–∞
        garbage_patterns = [
            (r'[*]{3,}', "***", "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–≤—ë–∑–¥–æ—á–∫–∏"),
            (r'[-]{3,}', "---", "–õ–∏–Ω–∏–∏ –∏–∑ –¥–µ—Ñ–∏—Å–æ–≤"),
            (r'\|{2,}', "|||", "–í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–µ –ª–∏–Ω–∏–∏"),
            (r'~{3,}', "~~~", "–¢–∏–ª—å–¥—ã"),
            (r'[*]{2}[@][*]{2}', "**@**", "–ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã"),
            (r'[*]{4}[*]{4}', "****", "–ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã"),
        ]

        for pattern, example, desc in garbage_patterns:
            if re.search(pattern, text):
                # –ò—â–µ–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
                matches = re.findall(pattern, text)
                for match in matches[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 –ø—Ä–∏–º–µ—Ä–∞
                    self.artifacts.append({
                        "type": "GARBAGE",
                        "ocr_text": match,
                        "expected_text": "",
                        "context": text[max(0, text.find(match)-20):text.find(match)+20],
                        "suggestion": f"–£–¥–∞–ª–∏—Ç—å: {desc}"
                    })

    def analyze_diacritics(self, locale: str, raw_ocr: dict, gt_data: dict):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Ç–µ—Ä—é –¥–∏–∞–∫—Ä–∏—Ç–∏–∫–∏."""
        diacritics_by_locale = {
            "pl_PL": "ƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈ºƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª",
            "cs_CZ": "√°ƒçƒè√©ƒõ√≠≈à√≥≈ô≈°≈•√∫≈Ø√Ω≈æ√Åƒåƒé√âƒö√ç≈á√ì≈ò≈†≈§√ö≈Æ√ù≈Ω",
            "de_DE": "√§√∂√º√ü√Ñ√ñ√ú",
            "es_ES": "√°√©√≠√≥√∫√º√±√ß√Å√â√ç√ì√ö√ú√ë√á",
        }

        diacritics = diacritics_by_locale.get(locale)
        if not diacritics:
            return

        ocr_text = raw_ocr.get("full_text", "")
        gt_items = gt_data.get("items", [])

        for item in gt_items:
            name = item.get("raw_name", item.get("name", ""))
            if not any(d in name for d in diacritics):
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –¥–∏–∞–∫—Ä–∏—Ç–∏–∫–∞ –≤ OCR
            found, matched, _ = self.fuzzy_find_in_text(name, ocr_text, threshold=0.8)

            if found:
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∏–∞–∫—Ä–∏—Ç–∏–∫–∏
                for d in diacritics:
                    if d in name and d not in matched:
                        self.artifacts.append({
                            "type": "DIAC_LOSS",
                            "ocr_text": matched,
                            "expected_text": name,
                            "context": name,
                            "suggestion": f"–ü–æ—Ç–µ—Ä—è –¥–∏–∞–∫—Ä–∏—Ç–∏–∫–∏: {d}"
                        })

    def analyze_script_mix(self, locale: str, raw_ocr: dict, gt_data: dict):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–º–µ—à–µ–Ω–∏–µ –∞–ª—Ñ–∞–≤–∏—Ç–æ–≤ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞/–ª–∞—Ç–∏–Ω–∏—Ü–∞)."""
        if locale not in ["uk_UA", "bg_BG", "ru_RU"]:
            return

        # –ü–∞—Ä—ã –ø–æ—Ö–æ–∂–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        script_pairs = [
            ("a", "–∞"), ("e", "–µ"), ("o", "–æ"), ("p", "—Ä"),
            ("c", "—Å"), ("x", "—Ö"), ("i", "—ñ"), ("y", "—É"),
            ("B", "–í"), ("M", "–ú"), ("H", "–ù"), ("P", "–†"),
            ("T", "–¢"), ("K", "–ö")
        ]

        ocr_text = raw_ocr.get("full_text", "")
        gt_items = gt_data.get("items", [])

        for item in gt_items:
            name = item.get("raw_name", item.get("name", ""))

            # –ò—â–µ–º –≤ OCR
            found, matched, _ = self.fuzzy_find_in_text(name, ocr_text, threshold=0.7)

            if found:
                for latin, cyrillic in script_pairs:
                    if latin in matched and cyrillic in name:
                        self.artifacts.append({
                            "type": "SCRIPT_MIX",
                            "ocr_text": matched,
                            "expected_text": name,
                            "context": matched,
                            "suggestion": f"–ó–∞–º–µ–Ω–∞ –ª–∞—Ç–∏–Ω—Å–∫–æ–π {latin} –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫—É—é {cyrillic}"
                        })
                    elif cyrillic in matched and latin in name:
                        self.artifacts.append({
                            "type": "SCRIPT_MIX",
                            "ocr_text": matched,
                            "expected_text": name,
                            "context": matched,
                            "suggestion": f"–ó–∞–º–µ–Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–æ–π {cyrillic} –Ω–∞ –ª–∞—Ç–∏–Ω—Å–∫—É—é {latin}"
                        })

    def analyze(self) -> dict:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–µ–∫–∞."""
        print(f"\n{'='*60}")
        print(f"–ê–Ω–∞–ª–∏–∑ —á–µ–∫–∞: {self.receipt_id}")
        print(f"{'='*60}\n")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        raw_ocr, gt_data = self.load_data()
        if not raw_ocr:
            return {"error": "raw_ocr not found"}
        if not gt_data:
            print("‚ö†Ô∏è  Ground Truth –Ω–µ –Ω–∞–π–¥–µ–Ω, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é –±–µ–∑ —ç—Ç–∞–ª–æ–Ω–∞\n")
            self.analyze_garbage(raw_ocr)
            return self._format_result()

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.extract_metadata(raw_ocr, gt_data)
        print(f"–õ–æ–∫–∞–ª—å: {self.metadata['locale']}")
        print(f"–ú–∞–≥–∞–∑–∏–Ω: {self.metadata['store']}")
        print(f"–°–ª–æ–≤ –≤ OCR: {self.metadata['total_words']}")
        print(f"–¢–æ–≤–∞—Ä–æ–≤ –≤ GT: {self.metadata['total_items']}\n")

        # –ê–Ω–∞–ª–∏–∑
        self.analyze_item_names(raw_ocr, gt_data)
        self.analyze_prices(raw_ocr, gt_data)
        self.analyze_garbage(raw_ocr)
        self.analyze_diacritics(self.metadata['locale'], raw_ocr, gt_data)
        self.analyze_script_mix(self.metadata['locale'], raw_ocr, gt_data)

        return self._format_result()

    def _format_result(self) -> dict:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–∏–¥."""
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º
        by_type = {}
        for artifact in self.artifacts:
            art_type = artifact["type"]
            if art_type not in by_type:
                by_type[art_type] = []
            by_type[art_type].append(artifact)

        return {
            "id": f"{self.metadata.get('locale', 'unknown')}_{self.receipt_id}",
            "receipt_id": self.receipt_id,
            "locale": self.metadata.get("locale", "unknown"),
            "store": self.metadata.get("store", "unknown"),
            "source_file": self.metadata.get("source_file", self.receipt_id),
            "artifacts": self.artifacts,
            "statistics": {
                "total_words": self.metadata.get("total_words", 0),
                "artifacts_found": len(self.artifacts),
                "by_type": {k: len(v) for k, v in by_type.items()}
            }
        }

    def print_summary(self):
        """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É."""
        if not self.artifacts:
            print("‚úÖ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return

        by_type = {}
        for a in self.artifacts:
            by_type[a["type"]] = by_type.get(a["type"], 0) + 1

        print(f"\n–ù–∞–π–¥–µ–Ω–æ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤: {len(self.artifacts)}")
        print("–ü–æ —Ç–∏–ø–∞–º:")
        for art_type, count in sorted(by_type.items(), key=lambda x: x[1], reverse=True):
            print(f"  {art_type}: {count}")

        print("\n–ü—Ä–∏–º–µ—Ä—ã –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤:")
        for i, artifact in enumerate(self.artifacts[:10], 1):
            print(f"\n{i}. [{artifact['type']}]")
            print(f"   OCR: {artifact['ocr_text']}")
            print(f"   –û–∂–∏–¥–∞–ª–æ—Å—å: {artifact.get('expected_text', '-')}")
            print(f"   –ü–æ–¥—Å–∫–∞–∑–∫–∞: {artifact['suggestion']}")


def main():
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python scripts/analyze_ocr_artifacts.py <receipt_id>")
        print("\n–ü—Ä–∏–º–µ—Ä—ã:")
        print("  python scripts/analyze_ocr_artifacts.py IMG_1292")
        print("  python scripts/analyze_ocr_artifacts.py PL_001")
        sys.exit(1)

    receipt_id = sys.argv[1]
    analyzer = ArtifactAnalyzer(receipt_id)
    result = analyzer.analyze()
    analyzer.print_summary()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    output_path = PROJECT_ROOT / "data" / "analysis" / "ocr_artifacts" / f"{receipt_id}.json"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")


if __name__ == "__main__":
    main()
